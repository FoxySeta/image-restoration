\documentclass[a4paper]{article}
\usepackage[italian]{babel}
\usepackage{epigraph}
\usepackage{float}
\usepackage{hhline}
% plotting
\usepackage{pgf}

\title{
  Ricostruzione di immagini \\ \large Relazione del progetto per
  l'insegnamento di Calcolo numerico
}
\author{
  R. Gianmaria,
  L. Tagliavini,
  S. Volpe
}

\date{
	Universit\`a di Bologna \\
  \today
}

\begin{document}

\maketitle
\thispagestyle{empty}

\pagebreak
\setcounter{page}{1}
\pagenumbering{roman}
\tableofcontents
\pagebreak
\pagenumbering{arabic}

\epigraph{Lo scopo del calcolo è la conoscenza, non i numeri.}
{\textit{R. W. Hamming}}
\epigraph{... ma per lo studente, i numeri sono spesso la via migliore verso la
conoscenza}
{\textit{A. Ralston}}

\section{Introduzione}
La rappresentabilità delle immagini digitali in forma matriciale fa sì che la
loro manipolazione possa trarre beneficio dagli strumenti del calcolo numerico:
un problema di interesse pratico è la ricostruzione di tali immagini quando
degradate. Nell'esperimento che segue, ci proponiamo di effettuare un'analisi
comparata di esempi di implementazioni dei metodi più noti per la risoluzione
di questo tipo di problemi. Assumeremo, fra le conoscenze pregresse della
lettrice e del lettore, il modello di formazione-registrazione di un'immagine
digitale e i principali metodi di regolarizzazione.

\section{Strumenti}
Tutte le procedure necessarie all'esperimento sono state implementate in Python
3. In particolare, viene fatto uso dei pacchetti:
\begin{itemize}
  \item \verb!matplotlib! per la creazione di grafici;
  \item \verb!numpy! per l'uso di matrici;
  \item \verb!scipy! per risolvere i problemi di ottimizzazione;
  \item \verb!skimage! per il calcolo delle metriche delle immagini.
\end{itemize}

\section{Procedimento}

\subsection{Generazione dell'insieme dei dati}
Sono state preparate otto immagini (da \verb!1.png! a \verb!8.png!) $512 \times
512$, in scala di grigi e contenenti dai due ai sei oggetti geometrici a tinta
unita su sfondo nero. Oltre a queste, vengono usate due foto reali, sempre in
scala di grigi ma di risoluzione $2048 \times 2048$: \verb!A.png! e
\verb!B.png!.

\subsection{Corruzione dell'insieme dei dati}
Per ogni immagine $x$ fra quelle sopra indicate, viene dapprima applicata una
sfocatura di matrice associata $A$ e poi un rumore uniforme di matrice associata
$\eta$ (entrambi costruiti a partire da una gaussiana) al fine di ottenere
un'immagine degradata $b$:
\begin{equation}\label{eq:degradation}
  b = Ax + \eta
\end{equation}
Tali operazioni sono ripetute più volte, ciascuna con valori diversi assegnati
ai parametri in questione (si veda \ref{results}).

\subsection{Risoluzione ingenua}
Una semplice ricostruzione $x^*$ viene implementata a partire da:
\begin{equation}\label{eq:naive}
  x^* = \arg \min_{x} \frac{1}{2}||Ax - b||^2_2
\end{equation}

\subsection{Risoluzione usando un termine di regolarizzazione di Tikhonov}
Introducendo un termine di regolarizzazione di Tikhonov, la \ref{eq:naive}
diventa:
\begin{equation}
  x^* = \arg \min_{x} \frac{1}{2}||Ax - b||^2_2 + \frac{\lambda}{2}||x||^2_2
\end{equation}

\subsection{Risoluzione usando la variazione totale}
Usando invece la variazione totale come termine di fattorizzazione, la
\ref{eq:naive} diventa:
\begin{equation}
  x^* = \arg \min_{x} \frac{1}{2}||Ax - b||^2_2 +
  \lambda\sum_i^n\sum_j^m\sqrt{||\nabla x(i,j)||_2^2+\epsilon^2}
\end{equation}

\section{Risultati}\label{results}
Dove non specificato, nei risultati che seguono viene usato il metodo dei
gradienti coniugati (GC) anziché quello del gradiente (G). 

\subsection{Confronto}
In figura \ref{fig:comparison} sono riportati i risultati dei diversi metodi di
ricostruzione su una immagine geometrica e due fotografie.
Si osserva che la differenza fra diverse versioni di una stessa immagine è molto
più evidente in \verb!1.png! che negli altri casi: qui, la ricostruzione più
"rumorosa", nonché di peggior qualità, è visibilmente quella naïve, mentre la
ricostruzione con variazione totale è quella che offre effetti più regolari.
\begin{figure}[H]
  \begin{center}
    \scalebox{0.75}{\input{methods.pgf}}
  \end{center}
  \caption{confronto dei risultati con parametro di regolarizzazione (dove
  usato) pari a $0.04$. Gli indici di PSNR e MSE non si riferiscono ad alcuna
  immagine presente, bensì alle degradazioni.}
  \label{fig:comparison}
\end{figure}
\subsection{Variazione dei parametri}
Variando i parametri in gioco, è possibile tabulare PSNR e MSE ottenuti con i
vari metodi: il risultato di questa operazione è mostrato in figura
\ref{fig:parameters}.
Confrontando celle diverse, ci si accorge che a valori del PSNR più alti
corrispondono sempre valori dell'MSE più bassi e viceversa: ci limitamo quindi
all'osservazione del primo di queste due grandezze. A parità di parametri, esso
cresce all'aumentare dell'indice della colonna: in alcune applicazioni del
metodo naïve vengono addirittura raggiunti valori negativi. All'aumentare
dell'intensità del rumore, i valori scendono, mentre nel caso del parametro di
regolarizzazione (fatta eccezione per la prima colonna), l'andamento del PSNR
sembra avere un atteggiamento concavo.
\begin{figure}[H]
  \begin{center}
    \scalebox{0.65}{\input{vars-blur.tex}}
  \end{center}
  \begin{center}
    \scalebox{0.65}{\input{vars-noise.tex}}
  \end{center}
  \begin{center}
    \scalebox{0.65}{\input{vars-lambda.tex}}
  \end{center}
  \caption{antologia di tabelle che confrontano le coppie ordinate (PSNR, MSE)
  di varie ricostruzioni della prima immagine di prova al variare della
  sfocatura, dell'intensità del rumore e del parametro di regolarizzazione
  usato.}
  \label{fig:parameters}
\end{figure}

\subsection{Dati aggregati}
La tabella \ref{fig:aggregation} riassume PSNR e MSE dell'intero insieme di
immagini geometriche tramite media e deviazione standard.
Si osserva che la deviazione standard del PSNR si minimizza per valori bassi del
PSNR stesso.
\begin{figure}[H]
  \begin{center}
    \scalebox{0.65}{\input{aggregations.tex}}
  \end{center}
  \caption{medie e deviazioni standard delle coppie ordinate (PSNR, MSE) al
  variare dei parametri. Il campione preso in esame è composto dalle
  ricostruzioni effettuate su ciascuna immagine geometrica dell'insieme di
  dati. Il metodo usato è quello dei gradienti coniugati, mentre la variazione
  totale è stata scelta come termine di regolarizzazione.}
  \label{fig:aggregation}
\end{figure}

\subsection{Esecuzioni}
Concludiamo confrontando, iterazione per iterazione, le esecuzioni del metodo
del gradiente e di quello dei gradienti coniugati su \verb!1.png!; il termine di
regolarizzazione usato è quello della variazione totale. Si consultino le figure
\ref{fig:iterations-error}, \ref{fig:iterations-objective} e
\ref{fig:iterations-gradient}. In tutti i casi le funzioni convergono dall'alto
a un valore basso. Nel caso dello studio del valore della funzione obiettivo e
della norma del suo gradiente, i valori a cui i due metodi convergono
approssimativamente coincidono. Nel caso dell'errore relativo e del valore della
funzione obiettivo, i grafici sono monotoni non crescenti per entrambi i metodi.
In tutti i casi, il metodo dei gradienti coniugati converge appena qualche
iterazione prima del metodo del gradiente.
\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \scalebox{0.4}{\input{iterations-error.pgf}}
    \label{fig:iterations-error}
    \caption{errore relativo dei due metodi a confronto}
  \end{minipage}
  \,\,
  \begin{minipage}[t]{0.48\linewidth}
    \centering
    \scalebox{0.4}{\input{iterations-objective.pgf}}
    \label{fig:iterations-objective}
    \caption{valori della funzione obiettivo da minimizzare a confronto}
  \end{minipage}
\end{figure}
\begin{figure}[H]
  \centering
  \begin{minipage}[t]{0.48\linewidth}
  \scalebox{0.4}{\input{iterations-objective.pgf}}
  \label{fig:iterations-gradient}
  \caption{norme dei gradienti della funzione obiettivo da minimizzare a
    confronto}
  \end{minipage}
\end{figure}

\section{Conclusioni}

\subsection{Confronto}
I motivi per cui \verb!1.png!, rispetto alle fotografie tradizionali, si presta
meglio al confronto visivo sono probabilmente due: una stesura omogenea del
colore, che permette più facilmente di individuare rumore, e bordi
geometricamente regolari, lungo i quali eventuali imperfezioni risultano più
visibili. L'uso di semplici immagini geometriche a tinta unita si rivela quindi
essere strategico per l'indagine in questione. Nella fattispecie, il rumore
della sua ricostruzione naïve è spiegabile tenendo conto che \ref{eq:naive} è la
riformulazione ai minimi quadrati di
\begin{equation}
  Ax = b
\end{equation}
la quale, a differenza di \ref{eq:degradation}, non tiene conto del rumore
$\eta$, e soffre quindi di una incertezza detta "rumore inverso". Il fatto che
il problema sia malcondizionato aggrava questa mancanza. Nel caso della
ricostruzione con variazione totale, invece, una possibile spiegazione per la
estrema regolarità osservata potrebbe ipotizzare che la funzione della
variazione totale imponga derivate parziali piccole nella soluzione ottenuta
penalizzando quelle grandi in modulo. Per quanto riguarda \verb!A.png! e
\verb!B.png!, invece, sappiamo per certo di star lavorando con immagini già
imperfette a priori del nostro processo di degradazione (di cui almeno
conosciamo la natura): vengono quindi a sommarsi errori provenienti da fonti
diverse. Queste fotografie non costituiscono un caso di studio semplice,
ne' adatto ai nostri scopi.

\subsection{Variazione dei parametri}
Il fatto che il PSNR cresca al diminuire di MSE è confermato dalla definizione
di PSNR:
\begin{equation}\label{eq:psnr}
  PSNR = 20 \log_{10}(\frac{\max{x}}{\sqrt{MSE}})
\end{equation}
dove $x$ è l'immagine originale. Anche nell'esposizione di queste conclusioni,
dunque, eviteremo di ripetere considerazioni speculari per queste due metriche.
Il crescere del PSNR all'aumentare dell'indice della colonna indica che, su
tutte le nostre prove, ciascuna delle strategie utilizzate ha dato risultati
migliori delle precedenti, e la nostra presentazione teorica segue quindi un
ordine di qualità crescente delle ricostruzioni, a partire dalla risoluzione
naïve che quindi ottiene i PSNR peggiori. Il fatto che talvolta questi
presentino addirittua segno negativo ha un'interessante intepretazione.
Imponiamo:
\begin{equation}
  PSNR < 0
\end{equation}
Facendo uso di \ref{eq:psnr}, arriviamo a:
\begin{equation}
  \max{x^2} < MSE
\end{equation}
o anche
\begin{equation}
  \forall x_{i,j} \in x \quad x_{i,j}^2 < MSE
\end{equation}
Questa relazione potrebbe venir letta come "nel caso di PSNR negativo, in tutti
i punti dell'immagine, l'errore quadratico medio sovrasta perfino il valore
quadratico dell'immagine originale". Cerchiamo ora di dare una spiegazione alla
relazione fra alcuni parametri e il risultato. Avere ottenuto ricostruzioni di
peggior qualità quando il rumore si faceva più intenso era prevedibile: nel caso
generale, aumentare l'incertezza sui dati porta a soluzioni anch'esse meno
accurate in una qualche misura, e nella nostro particolare situazione stiamo
lavorando con problemi tipicamente mal condizionati. Il carattere concavo del
PSNR al variare del parametro di regolarizzazione (dove questo viene
effettivamente usato), invece, suggerisce l'esistenza di un valore intermedio
che, se assegnato al parametro di regolarizzazione, massimizzi il PSNR. Questa
osservazione è in accordo con l'idea che la scelta del parametro sia frutto di
una sorta di "compromesso" fra valori troppo bassi (che causano una soluzione
molto sensibile al rumore) e troppo alti (che portano a una soluzione che non
riproduce adeguatamente i dati).

\subsection{Dati aggregati}
Lo studio dei valori medi, confermando i risultati sopra ottenuti, ne garantisce
l'indipendenza dalle peculiarità della sola immagine \verb!1.png!.

\subsection{Esecuzioni}
La convergenza dall'alto a un valore basso che abbiamo osservato è una
caratteristica desiderabile per tutti e tre gli indici: indica che, iterazione
dopo iterazione, entrambi gli algoritmi lavorano per avvicinarsi il più
possibile all'immagine originale (errore relativo), risolvendo il problema di
ottimizzazione (valore della funzione obiettivo) e cercando un punto che
soddisfi le condizioni necessarie al prim'ordine per un estremo relativo (norma
del gradiente nulla). Anche la coincidenza dei valori di convergenza per la
funzione obiettivo è desiderabile, perché mostra che i due algoritmi, seppur
diversi, sono in accordo sul valore della soluzione del problema di
ottimizzazione. L'errore relativo delle ricostruzioni rispetto all'immagine
originale, però, non coincide: la funzione obiettivo scelta, quindi, non è un
indice impeccabile della distanza fra una ricostruzione e l'immagine originale,
per quanto questa sia ovviamente una qualità desiderabile. Un chiaro ostacolo
alla realizzazione di questo scenario ideale è il rumore pseudoaleatorio
applicato all'immagine originale durante il processo di degradazione. La
decrescenza del grafico dei valori della funzione obiettivo era prevedibile,
perché a ogni iterazione vengono sempre scelte direzioni di discesa.

\subsection{Ricerche future}
Ulteriori ricerche sperimentali in questo ambito potrebbero includere:
\begin{itemize}
  \item immagini non in scala di grigi;
  \item l'uso di metodi per stabilire plausibili parametri di regolarizzazione
    (come il principio della discrepanza di Morozov);
  \item regolarizzazioni effettuate con norme diverse dalla norma 2;
  \item considerazioni sul costo computazionale del codice eseguito con
    conseguenti ottimizzazioni.
\end{itemize}
  
\end{document}
